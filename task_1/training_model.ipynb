{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5772a15-1d8e-43a4-9edd-8c019a35d9f2",
   "metadata": {},
   "source": [
    "So for this problem I also used pretty straightforward approach where I used ChatGPT to generate a lot of different textes about mountains in all around the world. Then I used doccano to maniually label all texts and saved labeled data. Then I proceeded to use DistilBert for my NER to detect mountain names in the text. To use it correctly, and for it to be more efficient i've manually done some hyperparams tuning. Although it helps a lot to find a balance between over and underfitting, the model still lack some tuning and maybe larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df83580b-1a48-49c1-a490-434f87bd35cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rost\\PycharmProjects\\NER\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForTokenClassification, Trainer, TrainingArguments, pipeline\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7da3d4d-d7a3-4974-8a49-c506b7f7a6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountainDataset(Dataset):\n",
    "    def __init__(self, texts, labels, max_length=512):\n",
    "        self.tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "        # Create an array to hold the labels\n",
    "        label_array = torch.ones(self.max_length, dtype=torch.long) * -100  # Use -100 to ignore index during loss computation\n",
    "        label_array[:len(labels)] = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}  # Remove extra dimension\n",
    "        item['labels'] = label_array\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "097ed8e7-4d0e-4d46-90e5-3fa0dd0ee91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"Load and process the data from a JSONL file for NER.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The path to the JSONL file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of texts (sentences).\n",
    "        list: A corresponding list of word-level labels.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            text = data['text']\n",
    "            words = text.split()\n",
    "            word_labels = [0] * text.count(\" \")  # Initialize all labels to 0\n",
    "\n",
    "            for label in data['label']:\n",
    "                start, end, label_type = label\n",
    "                # Find the word index for start position\n",
    "                start_index = text[:start].count(' ')\n",
    "                end_index = text[:end].count(' ')\n",
    "\n",
    "                # Update labels for words within the entity\n",
    "                for i in range(start_index, end_index):\n",
    "                    word_labels[i] = 1 if label_type == \"Mountain\" else 0\n",
    "\n",
    "            texts.append(text)\n",
    "            labels.append(word_labels)\n",
    "\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "055e4226-bb41-4b8f-afb4-8ae989293edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_labels(labels, max_length, pad_value=-100):\n",
    "    \"\"\"Pad the label sequences to a maximum length.\n",
    "\n",
    "    Args:\n",
    "        labels (list): The list of label sequences.\n",
    "        max_length (int): The maximum length to pad to.\n",
    "        pad_value (int): The value used for padding.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of padded label sequences.\n",
    "    \"\"\"\n",
    "    padded_labels = []\n",
    "    for label in labels:\n",
    "        # Check if the label sequence is longer than max_length\n",
    "        if len(label) > max_length:\n",
    "            # Truncate the label sequence if it's too long\n",
    "            padded_label = label[:max_length]\n",
    "        else:\n",
    "            # Pad the label sequence if it's too short\n",
    "            padded_label = label + [pad_value] * (max_length - len(label))\n",
    "        padded_labels.append(padded_label)\n",
    "\n",
    "    return torch.tensor(padded_labels)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "092c3d5c-3d4b-4bdc-be42-cfa46231c940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rost\\AppData\\Local\\Temp\\ipykernel_4820\\377665513.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_array[:len(labels)] = torch.tensor(labels, dtype=torch.long)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 11:28, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.562008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.829908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.307200</td>\n",
       "      <td>0.738857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.307200</td>\n",
       "      <td>0.676184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.354300</td>\n",
       "      <td>0.641367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rost\\AppData\\Local\\Temp\\ipykernel_4820\\377665513.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_array[:len(labels)] = torch.tensor(labels, dtype=torch.long)\n",
      "C:\\Users\\Rost\\AppData\\Local\\Temp\\ipykernel_4820\\377665513.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_array[:len(labels)] = torch.tensor(labels, dtype=torch.long)\n",
      "C:\\Users\\Rost\\AppData\\Local\\Temp\\ipykernel_4820\\377665513.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_array[:len(labels)] = torch.tensor(labels, dtype=torch.long)\n",
      "C:\\Users\\Rost\\AppData\\Local\\Temp\\ipykernel_4820\\377665513.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_array[:len(labels)] = torch.tensor(labels, dtype=torch.long)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=105, training_loss=0.3411435672215053, metrics={'train_runtime': 694.4754, 'train_samples_per_second': 0.605, 'train_steps_per_second': 0.151, 'total_flos': 54874302013440.0, 'train_loss': 0.3411435672215053, 'epoch': 5.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example data (you'll replace this with your own dataset)\n",
    "texts, labels = load_data(\"labeled_dataset.jsonl\")\n",
    "padded_labels = pad_labels(labels, 256)\n",
    "\n",
    "# Create Custom trainer which will properly count class weights\n",
    "labels_flattened = np.array([label for sublist in padded_labels for label in sublist if label != -100])\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(labels_flattened), y=labels_flattened)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "loss_fct = CrossEntropyLoss(weight=class_weights_tensor)\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, padded_labels, test_size=0.2, random_state=42)\n",
    "dataset = MountainDataset(train_texts, train_labels)\n",
    "val_dataset = MountainDataset(val_texts, val_labels)\n",
    "\n",
    "# Load the model\n",
    "model = DistilBertForTokenClassification.from_pretrained('./results/checkpoint-105', num_labels=2)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.05,\n",
    "    learning_rate=0.001,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    # eval_steps=20,\n",
    "    save_strategy=\"epoch\",\n",
    "    # gradient_accumulation_steps=2\n",
    ")\n",
    "\n",
    "\n",
    "# Trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3109d0b2-940d-448a-ac13-547d03d962dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ee0f3-dd3a-4078-a4e8-6a956375a356",
   "metadata": {},
   "source": [
    "Some testing. As we can see only words \"the Alps\" is labeled as label_1. So program can see some mountains and detect them in text. However, it needs to be said that algorithm still gives a lot of wrong results and needs to be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a9778e52-ccc7-4c5e-b253-f1ee003f6255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'LABEL_0', 'score': 0.854722, 'index': 1, 'word': 'my', 'start': 0, 'end': 2}\n",
      "{'entity': 'LABEL_0', 'score': 0.9617946, 'index': 2, 'word': 'favourite', 'start': 3, 'end': 12}\n",
      "{'entity': 'LABEL_0', 'score': 0.96299255, 'index': 3, 'word': 'mountains', 'start': 13, 'end': 22}\n",
      "{'entity': 'LABEL_0', 'score': 0.8687076, 'index': 4, 'word': 'are', 'start': 23, 'end': 26}\n",
      "{'entity': 'LABEL_1', 'score': 0.7748349, 'index': 5, 'word': 'the', 'start': 27, 'end': 30}\n",
      "{'entity': 'LABEL_1', 'score': 0.8525415, 'index': 6, 'word': 'alps', 'start': 31, 'end': 35}\n",
      "{'entity': 'LABEL_0', 'score': 0.94497156, 'index': 7, 'word': 'also', 'start': 36, 'end': 40}\n",
      "{'entity': 'LABEL_0', 'score': 0.9702717, 'index': 8, 'word': 'in', 'start': 41, 'end': 43}\n",
      "{'entity': 'LABEL_0', 'score': 0.9781503, 'index': 9, 'word': 'terms', 'start': 44, 'end': 49}\n",
      "{'entity': 'LABEL_0', 'score': 0.8332724, 'index': 10, 'word': 'of', 'start': 50, 'end': 52}\n",
      "{'entity': 'LABEL_0', 'score': 0.9561972, 'index': 11, 'word': 'different', 'start': 53, 'end': 62}\n",
      "{'entity': 'LABEL_0', 'score': 0.9755702, 'index': 12, 'word': 'processors', 'start': 63, 'end': 73}\n",
      "{'entity': 'LABEL_0', 'score': 0.7609855, 'index': 13, 'word': 'i', 'start': 74, 'end': 75}\n",
      "{'entity': 'LABEL_0', 'score': 0.9461747, 'index': 14, 'word': 'think', 'start': 76, 'end': 81}\n",
      "{'entity': 'LABEL_0', 'score': 0.63765794, 'index': 15, 'word': '\"', 'start': 82, 'end': 83}\n",
      "{'entity': 'LABEL_0', 'score': 0.90504485, 'index': 16, 'word': 'el', 'start': 83, 'end': 85}\n",
      "{'entity': 'LABEL_0', 'score': 0.9656745, 'index': 17, 'word': '##br', 'start': 85, 'end': 87}\n",
      "{'entity': 'LABEL_0', 'score': 0.8973303, 'index': 18, 'word': '##us', 'start': 87, 'end': 89}\n",
      "{'entity': 'LABEL_0', 'score': 0.80187136, 'index': 19, 'word': '\"', 'start': 89, 'end': 90}\n",
      "{'entity': 'LABEL_0', 'score': 0.97069854, 'index': 20, 'word': 'are', 'start': 91, 'end': 94}\n",
      "{'entity': 'LABEL_0', 'score': 0.6981284, 'index': 21, 'word': 'the', 'start': 95, 'end': 98}\n",
      "{'entity': 'LABEL_0', 'score': 0.89028615, 'index': 22, 'word': 'worst', 'start': 99, 'end': 104}\n"
     ]
    }
   ],
   "source": [
    "# Testing the results\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForTokenClassification.from_pretrained('./results/checkpoint-21')\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example text\n",
    "example_text = \"My favourite mountains are the Alps also in terms of different processors I think \\\"Elbrus\\\" are the worst\"\n",
    "\n",
    "# Get predictions\n",
    "predictions = ner_pipeline(example_text)\n",
    "\n",
    "# Process and print predictions\n",
    "for prediction in predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d146354-76b7-4b5c-8e0b-3626b4bcfb80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a0049-7ba4-4465-8177-3148d3bf92a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
